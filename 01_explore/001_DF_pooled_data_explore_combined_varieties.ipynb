{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook best viewed here: https://nbviewer.jupyter.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to explore data of the pipeline where SNPs were called/filtered across varieties. We probably won't be using the combined data, so I'm keeping this records only\n",
    "\n",
    "This notebooke encompasses\n",
    "- sending files to start varscan_pipeline on server\n",
    "- maf filtering\n",
    "- LD pruning to get SNPs for structure estimation in GEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonimports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy over fastq and md5 files to compute canada server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = '/data/fastq/mengmeng/CoAdapTree_DouglasFir/received_2019_Sep10'\n",
    "fastqs = fs(DIR, pattern='.fastq')\n",
    "len(fastqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdtext = op.join(DIR, 'cp_to_graham_cmds.txt')\n",
    "with open(cmdtext, 'w') as o:\n",
    "    cmds = []\n",
    "    for fastq in fastqs:\n",
    "        cmds.append(f'rsync -avz {fastq} graham:/scratch/lindb/DF_pooled/')\n",
    "    o.write(\"%s\" % '\\n'.join(cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n"
     ]
    }
   ],
   "source": [
    "lview, dview = get_client('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exe(cmd):\n",
    "    import os\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-096e3f4e5dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwatch_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/programs/brandon_anaconda3/mypy/pythonimports/pythonimports.py\u001b[0m in \u001b[0;36mwatch_async\u001b[0;34m(jobs, phase)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "jobs = make_jobs(cmds, exe, lview)\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/fastq/mengmeng/CoAdapTree_DouglasFir/received_2019_Sep10/cp_to_graham_cmds.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmdtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in jobs:\n",
    "    x = j.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "needed = []\n",
    "for cmd in cmds:\n",
    "    fq = op.basename(cmd.split()[2])\n",
    "    for x in ['NS.1195.001.D707---D504.DF_p54_cap25_kit3_R1.fastq.gz',\n",
    "              'NS.1195.001.D707---D504.DF_p54_cap25_kit3_R2.fastq.gz',\n",
    "              'NS.1195.001.D707---D505.DF_p85_cap27_kit3_R1.fastq.gz']:\n",
    "        if fq == x:\n",
    "            needed.append(cmd)\n",
    "len(needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "jobs = make_jobs(needed, exe, lview)\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter VarScan output for natural pops only\n",
    "\n",
    "I ran the pipeline that included orchard pops, so I'm filtering based on natural pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n"
     ]
    }
   ],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from filter_VariantsToTable.py to only pull out baseline-filtered snps based on variety\n",
    "\n",
    "# I've copied this from 001_DF_pooled_data_explore where I separate on variety, but here I'm ...\n",
    "# ... hacking it so that 'variety' is just all natural pops\n",
    "\n",
    "# modifications are marked with ########## (other than imports)\n",
    "def pklload(path):\n",
    "    import pickle\n",
    "    pkl = pickle.load(open(path, 'rb'))\n",
    "    return pkl\n",
    "dview['pklload'] = pklload\n",
    "\n",
    "def get_varscan_names(df, tablefile):                                          ############ added tablefile arg\n",
    "    \"\"\"Convert generic sample/pool names from varscan to something meaningful.\"\"\"\n",
    "    print('renaming varscan columns ...')\n",
    "    import os \n",
    "    pool = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(tablefile))))          ############ added\n",
    "    parentdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(tablefile))))      ############\n",
    "    \n",
    "    # get order of samps used to create varscan cmds (same order as datatable)\n",
    "    samps = pklload(os.path.join(parentdir, f'{pool}/pkl_files/poolsamps.pkl'))[pool]                 ############ \n",
    "    # create a list of names that varscan gives by default\n",
    "    generic = ['Sample%s' % (i+1) for i in range(len(samps))]\n",
    "    # create a map between generic and true samp names\n",
    "    dic = dict((gen, samp) for (gen, samp) in zip(generic, samps))\n",
    "    # rename the columns in df\n",
    "    cols = []\n",
    "    for col in df:\n",
    "        if '.' in col:\n",
    "            gen, rest = col.split(\".\")\n",
    "            samp = dic[gen]\n",
    "            col = '.'.join([samp, rest])\n",
    "        cols.append(col)\n",
    "    df.columns = cols\n",
    "    return df\n",
    "dview['get_varscan_names'] = get_varscan_names\n",
    "\n",
    "def load_data(tablefile, variety):\n",
    "    \"\"\"\n",
    "    Load the VariantsToTable output.\n",
    "    \n",
    "    Positional arguments:\n",
    "    tablefile - path to VariantsToTable output - used to find ploidy etc\n",
    "    \n",
    "    Returns:\n",
    "    df - pandas.dataframe; VariantsToTable output\n",
    "    tf - basename of tablefile\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas \n",
    "    \n",
    "    tf = os.path.basename(tablefile)\n",
    "\n",
    "    # load the data, create a column with CHROM-POS for locusID\n",
    "    df = pandas.read_csv(tablefile, sep='\\t')\n",
    "    print(f'{tf} has {len(df.index)} rows (includes multiallelic)')\n",
    "    df['locus'] = [\"%s-%s\" % (contig, pos) for (contig, pos) in zip(df['CHROM'].tolist(), df['POS'].tolist())]\n",
    "    df = get_varscan_names(df, tablefile)\n",
    "    \n",
    "    # keep only columns for this variety\n",
    "    cols = [col for col in df.columns if '.' not in col or col.split(\".\")[0] in varlist[variety]]  ### added\n",
    "    df = df[[col for col in df.columns if col in cols]].copy()                                     ### added\n",
    "    \n",
    "    return df, tf\n",
    "dview['load_data'] = load_data\n",
    "\n",
    "def write_file(tablefile, df, tipe, variety):\n",
    "    import pandas\n",
    "    import os\n",
    "    \"\"\"Write filtered pandas.dataframe to file using args to create file name.\"\"\"\n",
    "#     newfile = tablefile.replace(\".txt\", f\"_{tipe}.txt\")                   ########## commented out\n",
    "    dirdict = {'both': 'combined_varieties'}\n",
    "    dname = os.path.basename(os.path.dirname(tablefile))\n",
    "    write_dir = os.path.dirname(os.path.dirname(tablefile)) + f\"/{dirdict[variety]}/{dname}_{variety}\"                  ########## added\n",
    "    print('write_dir = ', write_dir)\n",
    "    bname = os.path.basename(tablefile).replace(\".txt\", f\"_{tipe}_{variety}.txt\")##### added   \n",
    "    newfile = os.path.join(write_dir, bname)                              ########## added\n",
    "    print(f'{tipe}_path = ', newfile)                                     ########## added\n",
    "    \n",
    "    df.to_csv(newfile, index=False, sep='\\t')\n",
    "    print('finished filtering VariantsToTable file: %s' % newfile)\n",
    "dview['write_file'] = write_file\n",
    "\n",
    "def adjust_freqs(smalldf):\n",
    "    \"\"\"\n",
    "    For loci with REF=N, set freqs of pools with REF=N in GT to numpy.nan.\n",
    "    Set alt freqs with respect to the second alt allele.\n",
    "    \n",
    "    Positional arguments:\n",
    "    smalldf - pandas.dataframe; df with only REF=N\n",
    "    \n",
    "    Returns:\n",
    "    ndf - smalldf with adjusted freqs in zeroth row\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    import numpy\n",
    "    gtcols = [col for col in smalldf.columns if 'GT' in col]\n",
    "\n",
    "    for col in gtcols:\n",
    "        gt = smalldf.loc[1, col]\n",
    "        if isinstance(gt, str):\n",
    "            freqcol = col.split(\".\")[0] + '.FREQ'\n",
    "            if not gt == 'N/N':\n",
    "                freq = smalldf.loc[0, freqcol]\n",
    "                if isinstance(freq, str):\n",
    "                    if \"%\" in freq:\n",
    "                        newfreq = \"%s%%\" % (100 - float(freq.split(\"%\")[0]))\n",
    "                        smalldf.loc[0, freqcol] = newfreq\n",
    "            else:\n",
    "                # if gt = N/N, adjust to undefined\n",
    "                smalldf.loc[1, freqcol] = numpy.nan\n",
    "        gt2 = smalldf.loc[0, col]\n",
    "        if isinstance(gt2, str):\n",
    "            if gt == 'N/N':\n",
    "                # if gt = N/N, adjust to undefined\n",
    "                smalldf.loc[0, freqcol] = numpy.nan\n",
    "    return smalldf\n",
    "dview['adjust_freqs'] = adjust_freqs\n",
    "\n",
    "def get_refn_snps(df, tipe, ndfs=None):\n",
    "    \"\"\"\n",
    "    Isolate polymorphisms with REF=N but two ALT single nuleodite alleles.\n",
    "    \n",
    "    Positional arguments:\n",
    "    df - pandas.dataframe; current filtered VariantsToTable output\n",
    "    \n",
    "    Returns:\n",
    "    dfs - list of loci (pandas.dataframes) with REF=N and two ALT alleles, counts with respect to second ALT\n",
    "    ndfs - return from pandas.conat(dfs)\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    # as far as I can tell, crisp output from convert_pooled_vcf.py will not output REF = N\n",
    "    ndf = df[df['REF'] == 'N'].copy()\n",
    "    ndf = ndf[ndf['TYPE'] == tipe].copy()\n",
    "    ncount = table(ndf['locus'])\n",
    "    nloci = [locus for locus in ncount if ncount[locus] == 2]\n",
    "    ndf = ndf[ndf['locus'].isin(nloci)].copy()\n",
    "    dfs = []\n",
    "    for locus in uni(ndf['locus']):\n",
    "        smalldf = ndf[ndf['locus'] == locus].copy()\n",
    "        if len(smalldf.index) == 2:\n",
    "            smalldf.index = range(len(smalldf.index))\n",
    "            smalldf = adjust_freqs(smalldf)\n",
    "            smalldf.loc[0,'ALT'] = \"%s+%s\" % (smalldf.loc[0,'ALT'], smalldf.loc[1,\"ALT\"])\n",
    "            dfs.append(pandas.DataFrame(smalldf.loc[0,:]).T)\n",
    "    if len(dfs) > 0:\n",
    "        ndfs = pandas.concat(dfs)\n",
    "    return (dfs, ndfs)\n",
    "dview['get_refn_snps'] = get_refn_snps\n",
    "\n",
    "def keep_snps(df, tf):\n",
    "    \"\"\"\n",
    "    Count CHROM-POS (locus) and keep only those with one ALT.\n",
    "    \n",
    "    Positional arguments:\n",
    "    df - pandas.dataframe; currently filtered VariantsToTable output\n",
    "    tf - basename of path to VariantsToTable output\n",
    "    Returns:\n",
    "    df - pandas.dataframe; non-multiallelic-filtered VariantsToTable output\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    loccount = table(df['locus'])\n",
    "    goodloci = [locus for locus in loccount if loccount[locus] == 1]\n",
    "    print(f'{tf} has {len(goodloci)} good loci (non-multiallelic)')\n",
    "\n",
    "    # filter df for multiallelic (multiple lines), REF != N\n",
    "    df = df[df['locus'].isin(goodloci)].copy()\n",
    "    df = df[df['REF'] != 'N'].copy()\n",
    "    return df\n",
    "dview['keep_snps'] = keep_snps\n",
    "\n",
    "def filter_missing_data(df, tf, tipe):\n",
    "    \"\"\"\n",
    "    Remove loci with < 25% missing data.\n",
    "    Count numpy.nan in .FREQ col to assess % missing data.\n",
    "    \n",
    "    Positional arguments:\n",
    "    df - pandas.dataframe; VariantsToTable output\n",
    "    tf - str; basename of tablefile\n",
    "    tipe - str; one of either \"SNP\" or \"INDEL\"\n",
    "    \n",
    "    Returns:\n",
    "    df - pandas.dataframe; missing data-filtered VariantsToTable output\n",
    "    \"\"\"\n",
    "    import tqdm\n",
    "    import pandas\n",
    "    import math\n",
    "    freqcols = [col for col in df.columns if '.FREQ' in col]\n",
    "    copy = get_copy(df, freqcols)\n",
    "    keepers = []\n",
    "    # else statement for running single pos.path.(megagamtos.path.yte) through:\n",
    "    thresh = math.floor(0.25 * len(freqcols)) if len(freqcols) > 1 else 1\n",
    "    for locus in tqdm.tqdm(copy.columns):\n",
    "        # if there is less than 25% missing data:\n",
    "        # the only time x != x is when x is nan (fastest way to count it)\n",
    "        count = sum(1 for x in copy[locus] if x != x)\n",
    "        if count < thresh:\n",
    "            keepers.append(locus)\n",
    "    df = df[df.index.isin(keepers)].copy()\n",
    "    df.index = range(len(df.index))\n",
    "    return df\n",
    "dview['filter_missing_data'] = filter_missing_data\n",
    "\n",
    "def get_copy(df, cols):\n",
    "    \"\"\"\n",
    "    Transpose dataframe using specific columns (that will be index after transformation).\n",
    "    Doing so helps speed things up.\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    return df[cols].T.copy()\n",
    "dview['get_copy'] = get_copy\n",
    "\n",
    "def get_variety_freq_cutoffs(variety, ploidy):\n",
    "    \"\"\"\n",
    "    Use number of pops per variety to determine lowfreq, highfreq.\n",
    "    Differs from pipeline.\n",
    "    \"\"\"\n",
    "    lowfreq = 1/sum([popploidy for pop,popploidy in ploidy.items() if pop in varlist[variety]])\n",
    "    ###############                                               ##### note diffs with get_freq_cutoffs(tablefile)\n",
    "    highfreq = 1 - lowfreq\n",
    "    return lowfreq, highfreq\n",
    "dview['get_variety_freq_cutoffs'] = get_variety_freq_cutoffs\n",
    "\n",
    "def filter_freq(df, tf, tipe, tablefile, variety):\n",
    "    \"\"\"\n",
    "    Keep fixed loci.\n",
    "    \n",
    "    Positional arguments:\n",
    "    df - pandas.dataframe; VariantsToTable output\n",
    "    tablefile - path to VariantsToTable output - used to find ploidy etc\n",
    "    tf - str; basename of tablefile\n",
    "    tipe - str; one of either \"SNP\" or \"INDEL\"\n",
    "    \n",
    "    Returns:\n",
    "    df - pandas.dataframe; freq-filtered VariantsToTable output\n",
    "    \"\"\"\n",
    "    import tqdm\n",
    "    import pandas\n",
    "    import os\n",
    "    import math\n",
    "    # believe it or not, it's faster to do qual and freq filtering in two steps vs an 'and' statement\n",
    "#     lowfreq, highfreq = get_freq_cutoffs(tablefile)                                         ############ removed\n",
    "#     print(f'filtering for global frequency ({lowfreq}, {highfreq})...')                     ############ moved\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # prep for filtering\n",
    "    freqcols = [col for col in df.columns if '.FREQ' in col]\n",
    "    pool = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(tablefile))))     ############ changed\n",
    "    parentdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(tablefile)))) ############\n",
    "    ploidy = pklload(os.path.join(parentdir, f'{pool}/pkl_files/ploidy.pkl'))[pool]           ############\n",
    "    lowfreq, highfreq = get_variety_freq_cutoffs(variety, ploidy)                             ############ added\n",
    "    print(f'filtering for global frequency ({lowfreq}, {highfreq})...')                       ## moved from above\n",
    "    \n",
    "    # carry on with poolseq datas\n",
    "    filtloci = []\n",
    "    afs = []\n",
    "    copy = get_copy(df, freqcols)\n",
    "    for locus in tqdm.tqdm(copy.columns):\n",
    "        freqs = dict((samp.replace(\".FREQ\",\"\"),freq) for (samp,freq)\n",
    "                     in copy[locus].str.rstrip('%').astype('float').items()\n",
    "                     if not math.isnan(freq))  # faster than .str.rstrip('%').astype('float').dropna()\n",
    "        if len(freqs) > 0:  # avoid loci with all freqs masked (avoid ZeroDivisionError)\n",
    "            # calc globfreq using the samps/ploidy that are present for this locus\n",
    "            globfreq = sum([ploidy[samp]*(freq/100)\n",
    "                            for (samp,freq) in freqs.items()]) / sum([ploidy[samp] for samp in freqs])\n",
    "            if lowfreq <= globfreq <= highfreq:\n",
    "                filtloci.append(locus)\n",
    "                # since we're going in order of rows in df ...\n",
    "                # ... we can use afs to replace AF col later since we reduce df to filtloci\n",
    "                afs.append(globfreq)\n",
    "                # which is about 40x faster than: df.loc[locus, 'AF'] = globfreq\n",
    "    print(f'{tf} has {len(filtloci)} {tipe}s that have global MAF > {lowfreq*100}%')\n",
    "    df = df[df.index.isin(filtloci)].copy()\n",
    "    df.index = range(len(df.index))\n",
    "    df['AF'] = afs\n",
    "    return df\n",
    "dview['filter_freq'] = filter_freq\n",
    "\n",
    "def filter_qual(df, tf, tipe, tablefile, variety):\n",
    "    \"\"\"\n",
    "    mask freqs that have GQ < 20.\n",
    "    \n",
    "    Positional arguments:\n",
    "    df - pandas.dataframe; VariantsToTable output\n",
    "    tf - str; basename of tablefile\n",
    "    tipe - str; one of either \"SNP\" or \"INDEL\"\n",
    "    \n",
    "    Returns: pandas.dataframe; quality-filtered VariantsToTable output\n",
    "    - FREQ and GT are masked (numpy.nan) if GQ < 20\n",
    "    \"\"\"\n",
    "    import tqdm\n",
    "    import pandas\n",
    "    import numpy\n",
    "    gqcols = [col for col in df.columns if '.GQ' in col]\n",
    "    print(f'masking bad freqs for {len(gqcols)} pools...')\n",
    "    for col in tqdm.tqdm(gqcols):\n",
    "        freqcol = col.replace(\".GQ\", \".FREQ\")\n",
    "#         gtcol = col.replace(\".GQ\", \".GT\")  # pretty sure this is depricated\n",
    "        # badloci True if qual < 20\n",
    "#         df.loc[df[col] < 20, [freqcol, gtcol]] = np.nan\n",
    "        df.loc[df[col] < 20, freqcol] = numpy.nan\n",
    "\n",
    "    print('filtering for missing data ...')\n",
    "    df = filter_missing_data(df, tf, tipe)\n",
    "\n",
    "    if len(df.index) > 0:\n",
    "        print(f'{tf} has {len(df.index)} {tipe}s that have GQ >= 20 and < 25% missing data')\n",
    "        df = filter_freq(df, tf, tipe, tablefile, variety)\n",
    "        df.index = range(len(df.index))\n",
    "    else:\n",
    "        print(f'{tf} did not have any {tipe}s that have GQ >= 20 for >= 75% of pops' +\n",
    "              '\\nnot bothering to filter for freq')\n",
    "#         df = drop_freq_cols(df)\n",
    "    return df\n",
    "dview['filter_qual'] = filter_qual\n",
    "\n",
    "\n",
    "def main(tablefile, tipe='SNP', parentdir=None, ret=True, variety=None):   ########## changed default args\n",
    "    import sys\n",
    "    import pandas\n",
    "    import numpy\n",
    "    import math\n",
    "    import tqdm\n",
    "    import os\n",
    "    from collections import Counter\n",
    "    # load the data\n",
    "    df, tf = load_data(tablefile, variety)\n",
    "    \n",
    "    # filter only SNPs\n",
    "    df = df[df['TYPE'] == tipe].copy()\n",
    "\n",
    "    # determine loci with REF=N but biallelic otherwise\n",
    "    if tipe == 'SNP':\n",
    "        dfs, ndfs = get_refn_snps(df, tipe)\n",
    "\n",
    "        # determine which loci are multiallelic\n",
    "        df = keep_snps(df, tf)\n",
    "    \n",
    "    if len(df.index) == 0:\n",
    "        if ret is True:\n",
    "            return df\n",
    "        else:\n",
    "            # save\n",
    "            write_file(tablefile, df, tipe)\n",
    "\n",
    "    # add in loci with REF=N but biallelic otherwise\n",
    "    if tipe == 'SNP' and len(dfs) > 0:\n",
    "        print(f'{tf} has {len(ndfs.index)} biallelic {tipe}s with REF=N')\n",
    "        dfs.append(df)\n",
    "        df = pandas.concat(dfs)\n",
    "\n",
    "    # filter for quality and missing data\n",
    "    df.index = range(len(df.index))\n",
    "    if 'varscan' in tf and tipe == 'SNP':\n",
    "        # if we allow to continue for INDEL, each line is treated as a locus (not true for INDEL)\n",
    "        df = filter_qual(df, tf, tipe, tablefile, variety)\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "    # look for filtering options called at 00_start.py\n",
    "    if parentdir is not None and tipe == 'SNP':\n",
    "        # translate stitched (if called at 00_start)   ############## no need to translate for DF\n",
    "\n",
    "        # remove repeats (if called at 00_start) - want to remove repeats before paralogs\n",
    "        df = remove_repeats(df.copy(),\n",
    "                            parentdir,\n",
    "                            tablefile,\n",
    "                            os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(tablefile)))), ## added\n",
    "                            variety)  ###### added\n",
    "#                             op.basename(pooldir))  # commented out\n",
    "\n",
    "        # remove paralog SNPs (if called at 00_start)\n",
    "        df = remove_paralogs(df.copy(), parentdir, tablefile,\n",
    "                             os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(tablefile)))),## added\n",
    "                             variety) ###### added\n",
    "########################################################################################################\n",
    "\n",
    "        \n",
    "    if ret is True:\n",
    "        print('returning df')\n",
    "        return df\n",
    "    else:\n",
    "        # save\n",
    "        write_file(tablefile, df, tipe, variety)\n",
    "\n",
    "dview['main'] = main\n",
    "dview['uni'] = uni\n",
    "from pythonimports import table # in case I use 'table' in an iteration\n",
    "dview['table'] = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_paralogs(snps, parentdir, snpspath, pool, variety):\n",
    "    \"\"\"\n",
    "    Remove sites from snptable that are thought to have multiple gene copies align to this position.\n",
    "    \n",
    "    # assumes\n",
    "    # paralog file has 'CHROM' and 'locus' in the header (best if this is the only data, reads in quicker)\n",
    "    #   where CHROM is the reference chromosome/scaffold\n",
    "    #   where locus is hyphen-separated CHROM-POS\n",
    "    \n",
    "    # paralog file is created from calling SNPs on haplotype data as diploid\n",
    "    #   no need to worry about translating stiched -> unstitched if SNPs called on same reference.\n",
    "    \"\"\"\n",
    "    import os, pandas\n",
    "    parpkl = os.path.join(parentdir, f'{pool}/pkl_files/paralog_snps.pkl')\n",
    "    if os.path.exists(parpkl):\n",
    "        # read in paralogfile\n",
    "#         paralogdict = pklload(parpkl)                                            ############ commented out\n",
    "#         if paralogdict[pool] is not None:                                        ############ commented out\n",
    "        if True:                                                                   ########## added\n",
    "            print('Removing paralogs sites ...')\n",
    "#             paralogs = pd.read_csv(paralogdict[pool], sep='\\t')                  ############ commented out\n",
    "            refdir = '/data/database/DouglasFir_ref_genome'                        ############ added\n",
    "            paralogfile = os.path.join(refdir, 'DF_mega-varscan_all_bedfiles_SNP_paralog_snps.txt')# added\n",
    "            paralogs = pandas.read_table(paralogfile)                                  ############ added\n",
    "            # remove and isolate paralogs from snps\n",
    "            truths = snps['locus'].isin(paralogs['locus'])\n",
    "            found_paralogs = snps[truths].copy()\n",
    "            snps = snps[~truths].copy()\n",
    "            snps.index = range(len(snps.index))\n",
    "\n",
    "            # write paralogs to a file\n",
    "#             parafile = snpspath.replace(\".txt\", \"_PARALOGS.txt\")                 ########## commented out\n",
    "            dirdict = {'both': 'combined_varieties'}  ## added\n",
    "            dname = os.path.basename(os.path.dirname(snpspath))  ## added\n",
    "            write_dir = os.path.dirname(os.path.dirname(snpspath)) + f\"/{dirdict[variety]}/{dname}_{variety}\"  ## added  \n",
    "#             write_dir = os.path.dirname(snpspath) + f\"_{variety}\"                  ########## prev added, now commented\n",
    "            bname = os.path.basename(snpspath).replace(\".txt\", f\"_PARALOGS_{variety}.txt\")### added   \n",
    "            parafile = os.path.join(write_dir, bname)                              ########## added\n",
    "            print('paralog_path = ', parafile)                                     ########## added\n",
    "            \n",
    "            found_paralogs.to_csv(parafile, sep='\\t', index=False)\n",
    "            print(f'{os.path.basename(snpspath)} has {len(snps.index)} non-paralog SNPs')\n",
    "    return snps\n",
    "dview['remove_paralogs'] = remove_paralogs\n",
    "\n",
    "\n",
    "def remove_repeats(snps, parentdir, snpspath, pool, variety):\n",
    "    \"\"\"\n",
    "    Remove SNPs that are found to be in repeat-masked regions.\n",
    "    \n",
    "    # assumes\n",
    "    # that the positions have been translated BEFORE removing repeats\n",
    "        # took forever to create unstitched repeat regions, don't want to translate repeat file\n",
    "        # this way I can just use unstitched chrom if reference is stitched\n",
    "    # repeat file has a header ('CHROM', 'start', 'stop')\n",
    "    # start and stop positions of repeat regions are 1-based\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    import tqdm\n",
    "    import os\n",
    "    reppkl = os.path.join(parentdir, f'{pool}/pkl_files/repeat_regions.pkl')\n",
    "    if os.path.exists(reppkl):\n",
    "        # read in repeat regions\n",
    "#         repeatdict = pklload(reppkl)                                             ########## commented out\n",
    "#         if repeatdict[pool] is not None:                                         ########## commented out\n",
    "        if True:                                                                   ########## added\n",
    "            print('Removing repeat regions ...')\n",
    "            # if user selected translation be applied to this pool\n",
    "#             repeats = pd.read_csv(repeatdict[pool], sep='\\t')                    ########## commented out\n",
    "            repeats = pandas.read_table('/data/database/DouglasFir_ref_genome/DF_ref_edit_repeats.txt')   #### added\n",
    "            # figure out if data is from stitched or not\n",
    "            if 'unstitched_chrom' in snps.columns:\n",
    "                # then the snps have been translated: stitched -> unstitched\n",
    "                chromcol = 'unstitched_chrom'\n",
    "                poscol = 'unstitched_pos'\n",
    "                print('\\tsnps have been translated')\n",
    "            else:\n",
    "                # otherwise SNPs were called on unstitched reference\n",
    "                chromcol = 'CHROM'\n",
    "                poscol = 'POS'\n",
    "                print('\\tsnps have not been translated')\n",
    "            # reduce repeats to the chroms that matter (helps speed up lookups)\n",
    "            repeats = repeats[repeats['CHROM'].isin(snps[chromcol].tolist())].copy()\n",
    "\n",
    "            # isolate SNPs in repeat regions\n",
    "            repeat_snps = []\n",
    "            for chrom in tqdm.tqdm(uni(snps[chromcol])):\n",
    "                reps = repeats[repeats['CHROM'] == chrom].copy()\n",
    "                mysnps = snps[snps[chromcol] == chrom].copy()\n",
    "                if len(reps.index) > 0 and len(mysnps.index) > 0:\n",
    "                    for row in mysnps.index:\n",
    "                        pos = snps.loc[row, poscol]  # index is maintained from snps to mysnsps\n",
    "                        df = reps[reps['stop'].astype(int) >= int(pos)].copy()\n",
    "                        df = df[df['start'].astype(int) <= int(pos)].copy()\n",
    "                        if len(df.index) > 0:\n",
    "                            assert len(df.index) == 1\n",
    "                            repeat_snps.append(row)\n",
    "\n",
    "            # save repeats\n",
    "            print(f'\\tSaving {len(repeat_snps)} repeat regions')\n",
    "#             repeat_path = snpspath.replace(\".txt\", \"_REPEATS.txt\")               ########## comm ented out\n",
    "            dirdict = {'both': 'combined_varieties'}  ## added\n",
    "            dname = os.path.basename(os.path.dirname(snpspath))  ## added\n",
    "            write_dir = os.path.dirname(os.path.dirname(snpspath)) + f\"/{dirdict[variety]}/{dname}_{variety}\"  ## added  \n",
    "#             write_dir = os.path.dirname(snpspath) + f\"_{variety}\"                  ########## prev added, now commented\n",
    "            bname = os.path.basename(snpspath).replace(\".txt\", f\"_REPEATS_{variety}.txt\")### added   \n",
    "            repeat_path = os.path.join(write_dir, bname)                           ########## added\n",
    "            print('repeat_path = ', repeat_path)                                   ########## added\n",
    "            \n",
    "            myrepeats = snps[snps.index.isin(repeat_snps)].copy()\n",
    "            myrepeats.to_csv(repeat_path, sep='\\t', index=False)\n",
    "\n",
    "            # remove SNPs in repeat regions\n",
    "            snps = snps[~snps.index.isin(repeat_snps)].copy()\n",
    "            snps.index = range(len(snps.index))\n",
    "\n",
    "            print(f'{os.path.basename(snpspath)} has {len(snps.index)} SNPs outside of repeat regions')\n",
    "\n",
    "    return snps\n",
    "dview['remove_repeats'] = remove_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both 73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'both': ['DF_p1',\n",
       "  'DF_p2',\n",
       "  'DF_p3',\n",
       "  'DF_p4',\n",
       "  'DF_p5',\n",
       "  'DF_p6',\n",
       "  'DF_p7',\n",
       "  'DF_p8',\n",
       "  'DF_p9',\n",
       "  'DF_p10',\n",
       "  'DF_p11',\n",
       "  'DF_p12',\n",
       "  'DF_p13',\n",
       "  'DF_p14',\n",
       "  'DF_p15',\n",
       "  'DF_p16',\n",
       "  'DF_p17',\n",
       "  'DF_p18',\n",
       "  'DF_p19',\n",
       "  'DF_p20',\n",
       "  'DF_p23',\n",
       "  'DF_p24',\n",
       "  'DF_p25',\n",
       "  'DF_p26',\n",
       "  'DF_p27',\n",
       "  'DF_p28',\n",
       "  'DF_p29',\n",
       "  'DF_p30',\n",
       "  'DF_p31',\n",
       "  'DF_p32',\n",
       "  'DF_p33',\n",
       "  'DF_p34',\n",
       "  'DF_p35',\n",
       "  'DF_p36',\n",
       "  'DF_p37',\n",
       "  'DF_p38',\n",
       "  'DF_p39',\n",
       "  'DF_p40',\n",
       "  'DF_p41',\n",
       "  'DF_p42',\n",
       "  'DF_p43',\n",
       "  'DF_p44',\n",
       "  'DF_p45',\n",
       "  'DF_p46',\n",
       "  'DF_p47',\n",
       "  'DF_p48',\n",
       "  'DF_p49',\n",
       "  'DF_p50',\n",
       "  'DF_p51',\n",
       "  'DF_p52',\n",
       "  'DF_p53',\n",
       "  'DF_p54',\n",
       "  'DF_p55',\n",
       "  'DF_p56',\n",
       "  'DF_p57',\n",
       "  'DF_p58',\n",
       "  'DF_p59',\n",
       "  'DF_p60',\n",
       "  'DF_p61',\n",
       "  'DF_p62',\n",
       "  'DF_p72',\n",
       "  'DF_p73',\n",
       "  'DF_p74',\n",
       "  'DF_p75',\n",
       "  'DF_p76',\n",
       "  'DF_p77',\n",
       "  'DF_p78',\n",
       "  'DF_p79',\n",
       "  'DF_p80',\n",
       "  'DF_p81',\n",
       "  'DF_p82',\n",
       "  'DF_p83',\n",
       "  'DF_p84']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# envdata has variety ID - THIS IS CHANGED FROM WHEN SORTING INDIVIDUAL VARIETIES\n",
    "envdata = pd.read_table('/data/projects/pool_seq/environemental_data/df_ALL-naturalpops_std_env-19variables.txt')\n",
    "envdata = envdata[envdata['our_id']==envdata['our_id']]  # removes irrelevant pop with our_id=nan\n",
    "\n",
    "pool2var = {}\n",
    "varlist = {}\n",
    "for row in envdata.index:\n",
    "    pool = envdata.loc[row, 'our_id']\n",
    "#     variety = envdata.loc[row, 'Variety']\n",
    "    variety = 'both'   # THIS IS THE PART I CHANGED TO GRAB BOTH VARIETIES\n",
    "    pool2var[pool] = variety\n",
    "    if variety not in varlist:\n",
    "        varlist[variety] = []\n",
    "    varlist[variety].append(pool)\n",
    "for variety,pops in varlist.items():\n",
    "    print(variety, len(pops))\n",
    "dview['varlist'] = varlist\n",
    "varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both combined_varieties\n"
     ]
    }
   ],
   "source": [
    "# create directories to save files\n",
    "dirdict = {'both': 'combined_varieties'}\n",
    "for variety in varlist.keys():\n",
    "    print(variety, dirdict[variety])\n",
    "    makedir(f'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/{dirdict[variety]}/01_unfiltered_{variety}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0391_table.txt has 11890 rows (includes multiallelic)\n",
      "renaming varscan columns ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0391_table.txt has 10219 good loci (non-multiallelic)\n",
      "masking bad freqs for 73 pools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 720.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering for missing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10216/10216 [00:00<00:00, 20272.40it/s]\n",
      "  5%|▍         | 210/4380 [00:00<00:01, 2091.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0391_table.txt has 4380 SNPs that have GQ >= 20 and < 25% missing data\n",
      "filtering for global frequency (0.00017283097131005876, 0.99982716902869)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4380/4380 [00:02<00:00, 2117.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0391_table.txt has 4351 SNPs that have global MAF > 0.017283097131005877%\n",
      "Removing repeat regions ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsnps have not been translated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:05<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving 61 repeat regions\n",
      "repeat_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0391_table_REPEATS_both.txt\n",
      "DF_pooled_varscan_bedfile_0391_table.txt has 4290 SNPs outside of repeat regions\n",
      "Removing paralogs sites ...\n",
      "paralog_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0391_table_PARALOGS_both.txt\n",
      "DF_pooled_varscan_bedfile_0391_table.txt has 4285 non-paralog SNPs\n",
      "write_dir =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both\n",
      "SNP_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0391_table_SNP_both.txt\n",
      "finished filtering VariantsToTable file: /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0391_table_SNP_both.txt\n"
     ]
    }
   ],
   "source": [
    "# test out filtering\n",
    "tablefile = '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/01_unfiltered/DF_pooled_varscan_bedfile_0391_table.txt'\n",
    "df = main(tablefile, tipe='SNP', parentdir=\"/data/projects/pool_seq/DF_datasets/DF_pooled_GEA\",\n",
    "          ret=False, variety='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in dataframe and make sure the total pops make sense\n",
    "df = pd.read_table('/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0391_table_SNP_both.txt')\n",
    "len(varlist['both']), len([col for col in df.columns if 'FREQ' in col]) == len(varlist['both'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now do in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all of the varscan outputs\n",
    "files = fs('/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/01_unfiltered',\n",
    "           endswith='table.txt')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928\n",
      "932\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-591db97ded8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                               \u001b[0;34m'ret'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                               'variety':'both'}))\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwatch_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mypy/pythonimports/pythonimports.py\u001b[0m in \u001b[0;36mwatch_async\u001b[0;34m(jobs, phase, sleep)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# filter across both varieties\n",
    "jobs = []\n",
    "for f in files:\n",
    "    jobs.append(lview.apply_async(main, f, **{'tipe':'SNP',\n",
    "                                              'parentdir':\"/data/projects/pool_seq/DF_datasets/DF_pooled_GEA\",\n",
    "                                              'ret':False,\n",
    "                                              'variety':'both'}))\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0006_table.txt has 23483 rows (includes multiallelic)\n",
      "renaming varscan columns ...\n",
      "DF_pooled_varscan_bedfile_0006_table.txt has 20077 good loci (non-multiallelic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 649.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masking bad freqs for 73 pools...\n",
      "filtering for missing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20077/20077 [00:01<00:00, 19584.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0006_table.txt has 9801 SNPs that have GQ >= 20 and < 25% missing data\n",
      "filtering for global frequency (0.00017283097131005876, 0.99982716902869)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9801/9801 [00:04<00:00, 1994.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0006_table.txt has 9722 SNPs that have global MAF > 0.017283097131005877%\n",
      "Removing repeat regions ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 309.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsnps have not been translated\n",
      "\tSaving 0 repeat regions\n",
      "repeat_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0006_table_REPEATS_both.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0006_table.txt has 9722 SNPs outside of repeat regions\n",
      "Removing paralogs sites ...\n",
      "paralog_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0006_table_PARALOGS_both.txt\n",
      "DF_pooled_varscan_bedfile_0006_table.txt has 9722 non-paralog SNPs\n",
      "write_dir =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both\n",
      "SNP_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0006_table_SNP_both.txt\n",
      "finished filtering VariantsToTable file: /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0006_table_SNP_both.txt\n",
      "DF_pooled_varscan_bedfile_0090_table.txt has 28318 rows (includes multiallelic)\n",
      "renaming varscan columns ...\n",
      "DF_pooled_varscan_bedfile_0090_table.txt has 23770 good loci (non-multiallelic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 617.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masking bad freqs for 73 pools...\n",
      "filtering for missing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 23769/23769 [00:01<00:00, 19283.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0090_table.txt has 13035 SNPs that have GQ >= 20 and < 25% missing data\n",
      "filtering for global frequency (0.00017283097131005876, 0.99982716902869)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13035/13035 [00:06<00:00, 2013.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0090_table.txt has 12924 SNPs that have global MAF > 0.017283097131005877%\n",
      "Removing repeat regions ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsnps have not been translated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:09<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving 294 repeat regions\n",
      "repeat_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0090_table_REPEATS_both.txt\n",
      "DF_pooled_varscan_bedfile_0090_table.txt has 12630 SNPs outside of repeat regions\n",
      "Removing paralogs sites ...\n",
      "paralog_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0090_table_PARALOGS_both.txt\n",
      "DF_pooled_varscan_bedfile_0090_table.txt has 12630 non-paralog SNPs\n",
      "write_dir =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both\n",
      "SNP_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0090_table_SNP_both.txt\n",
      "finished filtering VariantsToTable file: /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0090_table_SNP_both.txt\n",
      "DF_pooled_varscan_bedfile_0337_table.txt has 21496 rows (includes multiallelic)\n",
      "renaming varscan columns ...\n",
      "DF_pooled_varscan_bedfile_0337_table.txt has 18495 good loci (non-multiallelic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 595.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masking bad freqs for 73 pools...\n",
      "filtering for missing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 18495/18495 [00:01<00:00, 17857.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0337_table.txt has 9049 SNPs that have GQ >= 20 and < 25% missing data\n",
      "filtering for global frequency (0.00017283097131005876, 0.99982716902869)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9049/9049 [00:04<00:00, 2020.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0337_table.txt has 8980 SNPs that have global MAF > 0.017283097131005877%\n",
      "Removing repeat regions ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/38 [00:00<00:01, 27.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsnps have not been translated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:10<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving 447 repeat regions\n",
      "repeat_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0337_table_REPEATS_both.txt\n",
      "DF_pooled_varscan_bedfile_0337_table.txt has 8533 SNPs outside of repeat regions\n",
      "Removing paralogs sites ...\n",
      "paralog_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0337_table_PARALOGS_both.txt\n",
      "DF_pooled_varscan_bedfile_0337_table.txt has 8533 non-paralog SNPs\n",
      "write_dir =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both\n",
      "SNP_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0337_table_SNP_both.txt\n",
      "finished filtering VariantsToTable file: /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0337_table_SNP_both.txt\n",
      "DF_pooled_varscan_bedfile_0507_table.txt has 25503 rows (includes multiallelic)\n",
      "renaming varscan columns ...\n",
      "DF_pooled_varscan_bedfile_0507_table.txt has 21744 good loci (non-multiallelic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 558.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masking bad freqs for 73 pools...\n",
      "filtering for missing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 21742/21742 [00:01<00:00, 17632.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0507_table.txt has 10456 SNPs that have GQ >= 20 and < 25% missing data\n",
      "filtering for global frequency (0.00017283097131005876, 0.99982716902869)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10456/10456 [00:05<00:00, 2010.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_pooled_varscan_bedfile_0507_table.txt has 10390 SNPs that have global MAF > 0.017283097131005877%\n",
      "Removing repeat regions ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsnps have not been translated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:12<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving 376 repeat regions\n",
      "repeat_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0507_table_REPEATS_both.txt\n",
      "DF_pooled_varscan_bedfile_0507_table.txt has 10014 SNPs outside of repeat regions\n",
      "Removing paralogs sites ...\n",
      "paralog_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0507_table_PARALOGS_both.txt\n",
      "DF_pooled_varscan_bedfile_0507_table.txt has 10014 non-paralog SNPs\n",
      "write_dir =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both\n",
      "SNP_path =  /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0507_table_SNP_both.txt\n",
      "finished filtering VariantsToTable file: /data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both/DF_pooled_varscan_bedfile_0507_table_SNP_both.txt\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(jobs):\n",
    "    if not j.ready():\n",
    "        main(files[i], **{'tipe':'SNP',\n",
    "                          'parentdir':\"/data/projects/pool_seq/DF_datasets/DF_pooled_GEA\",\n",
    "                          'ret':False,\n",
    "                          'variety':'both'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no errors\n",
    "for j in jobs:\n",
    "    x = j.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_unfiltered_both 0\n"
     ]
    }
   ],
   "source": [
    "# check to see how many files were produced per bedfile\n",
    "dirs = ['/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/01_unfiltered_both']\n",
    "for d in dirs:\n",
    "    filtfiles = fs(d, endswith='.txt')\n",
    "    bedtofiles = {}\n",
    "    for f in filtfiles:\n",
    "        bed = f.split(\"bedfile_\")[1].split(\"_table\")[0]\n",
    "        assert float(bed) == int(bed)\n",
    "        if bed not in bedtofiles:\n",
    "            bedtofiles[bed] = []\n",
    "        bedtofiles[bed].append(f)\n",
    "\n",
    "    missing = []\n",
    "    for i in range(int(max(bedtofiles.keys()))):\n",
    "        bed = str(i).zfill(4)\n",
    "        if not bed in bedtofiles.keys():\n",
    "            missing.append(files[i])\n",
    "        elif len(bedtofiles[bed]) != 3:\n",
    "            missing.append(files[i])\n",
    "    print(op.basename(d), len(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['both'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varlist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new dirs\n",
    "for variety in varlist:\n",
    "    makedir(f'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/{dirdict[variety]}/02_baseline_filtered_{variety}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n"
     ]
    }
   ],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(f):\n",
    "    import pandas\n",
    "    return pandas.read_table(f)\n",
    "\n",
    "def save_df(df, f):\n",
    "    import pandas\n",
    "    df.to_csv(f, sep='\\t', index=False)\n",
    "    \n",
    "def concat_files(d, dstdir, variety):\n",
    "    \"\"\"Read in filtered dataframes, concat into single df, write to file.\"\"\"\n",
    "    import pandas\n",
    "    import os\n",
    "    outfiles = []\n",
    "    lview,dview = get_client()\n",
    "    for tipe in ['PARALOGS', 'REPEATS', 'SNP']:\n",
    "        files = fs(d, pattern=tipe, endswith='.txt')\n",
    "        jobs = make_jobs(files, read_df, lview)\n",
    "        watch_async(jobs)\n",
    "        outfile = os.path.join(dstdir, f'DF_pooled-varscan_all_bedfiles_{tipe}_{variety}.txt')\n",
    "        outfiles.append(outfile)\n",
    "        file = os.path.join(outfile)\n",
    "        for i,j in enumerate(jobs):\n",
    "            if i == 0:\n",
    "                j.r.to_csv(file, sep='\\t', index=False)\n",
    "            else:\n",
    "                j.r.to_csv(file, sep='\\t', index=False, header=False, mode='a')\n",
    "        del jobs\n",
    "    return outfiles\n",
    "dview['fs'] = fs\n",
    "dview['make_jobs'] = make_jobs\n",
    "dview['get_client'] = get_client\n",
    "dview['watch_async'] = watch_async\n",
    "dview['read_df'] = read_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['both'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varlist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n",
      "932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseAsyncIOLoop._handle_events(83, 1)\n",
      "handle: <Handle BaseAsyncIOLoop._handle_events(83, 1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 139, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"<decorator-gen-153>\", line 2, in _dispatch_reply\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/site-packages/ipyparallel/client/client.py\", line 75, in unpack_message\n",
      "    return f(self, msg)\n",
      "  File \"/data/programs/brandon_anaconda3/envs/py37/lib/python3.7/site-packages/ipyparallel/client/client.py\", line 935, in _dispatch_reply\n",
      "    raise KeyError(\"Unhandled reply message type: %s\" % msg_type)\n",
      "KeyError: 'Unhandled reply message type: apply_request'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/02_baseline_filtered_both/DF_pooled-varscan_all_bedfiles_PARALOGS_both.txt',\n",
       " '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/02_baseline_filtered_both/DF_pooled-varscan_all_bedfiles_REPEATS_both.txt',\n",
       " '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/02_baseline_filtered_both/DF_pooled-varscan_all_bedfiles_SNP_both.txt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in dfs in parallel, write to final file\n",
    "variety = 'both'\n",
    "d = f'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/{dirdict[variety]}/01_unfiltered_{variety}'\n",
    "dstdir = f'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/{dirdict[variety]}/02_baseline_filtered_{variety}'\n",
    "assert op.exists(dstdir)\n",
    "assert op.exists(d)\n",
    "concat_files(d, dstdir, variety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('TATTATTTGGATCATTTATTTGTATTTACCCTTCAxATCATTCTCTTTCTCTCTGTAATCTCAGGATATGA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter for MAF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonimports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 56\n"
     ]
    }
   ],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipto_df(f, skipto, nrows, cols=None, filter_maf=False, **kwargs):\n",
    "    \"\"\"Retrieve dataframe in parallel so that all rows are captured when iterating.\n",
    "    \n",
    "    f = filename to open\n",
    "    skipto = row number to skip, read rows thereafter\n",
    "    nrows = how many rows to read from f after skipto\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    \n",
    "    if skipto == 0:\n",
    "        df = pandas.read_table(f, nrows=nrows-1)\n",
    "    else:\n",
    "        df = pandas.read_table(f, skiprows=range(1, skipto), nrows=nrows)\n",
    "    \n",
    "    if cols is not None:\n",
    "        if isinstance(cols, str):\n",
    "            cols = [cols]\n",
    "        df = df[cols].copy()\n",
    "    \n",
    "    if filter_maf is True:\n",
    "        return maf_filter(df, **kwargs)\n",
    "    \n",
    "    return df\n",
    "dview['get_skipto_df'] = get_skipto_df\n",
    "\n",
    "def maf_filter(chunk, maf=0.05, **kwargs):\n",
    "    \"\"\"filter minor allele frequency >= maf, create maf column, return df.\"\"\"\n",
    "    import pandas\n",
    "    import os\n",
    "\n",
    "    # filter for MAF\n",
    "    df = chunk[(chunk['AF'].astype(float) >= maf) & (chunk['AF'].astype(float) <= (1-maf))].copy()\n",
    "    # create MAF column\n",
    "    df['MAF'] = df['AF']\n",
    "    df.loc[df['AF'].astype(float) > 0.5, 'MAF'] = 1 - chunk['AF'][chunk['AF'].astype(float) > 0.5]\n",
    "    assert sum(df['MAF']<maf) == 0\n",
    "    \n",
    "    return df\n",
    "dview['maf_filter'] = maf_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirdict = {'both': 'combined_varieties'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both SNP 9846117\n",
      "both PARALOGS 1809\n",
      "both REPEATS 299010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/02_baseline_filtered_both/DF_pooled-varscan_all_bedfiles_SNP_both.txt': 9846117,\n",
       " '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/02_baseline_filtered_both/DF_pooled-varscan_all_bedfiles_PARALOGS_both.txt': 1809,\n",
       " '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/02_baseline_filtered_both/DF_pooled-varscan_all_bedfiles_REPEATS_both.txt': 299010}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get linenums for each variety and each type\n",
    "linenums = {}\n",
    "for variety in ['both']:\n",
    "    d = f'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/{dirdict[variety]}/02_baseline_filtered_{variety}'\n",
    "    assert op.exists(d)\n",
    "    for tipe in ['SNP', 'PARALOGS', 'REPEATS']:\n",
    "        f = op.join(d, f'DF_pooled-varscan_all_bedfiles_{tipe}_{variety}.txt')\n",
    "        out = !wc -l $f\n",
    "        linenums[f] = int(out[0].split()[0])-1\n",
    "        print(variety, tipe, linenums[f])\n",
    "linenums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### begin filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# read in SNPs in parallel\n",
    "dfs = {}\n",
    "for f in linenums:\n",
    "    if '_SNP_' not in f:\n",
    "        nrows = 50000\n",
    "        jobs = []\n",
    "        count = 0\n",
    "        for skipto in range(0, linenums[f], nrows):\n",
    "            num = str(count).zfill(4)\n",
    "            jobs.append(lview.apply_async(get_skipto_df, *(f, skipto, nrows), **{'filter_maf':True, 'maf':0.05}))\n",
    "            count += 1\n",
    "        watch_async(jobs)\n",
    "        dfs[f] = pd.concat([j.r for j in jobs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recalcuate RD\n",
    "\n",
    "Looking at our testdata (1 poolseq pop vs indSeq of same individuals), AD/DP was consistent with the frequency prediction from GATK. We saw that adjusting FREQ to AD / (AD + RD) decreased concordance between the two datasets. So that we are consistent with respect to uncorrected and corrected, I'm adjusting RD = DP - AD so we don't have to make adjustments in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalc_rd(df):\n",
    "    \"\"\"Recalculate RD so RD = DP - AD.\"\"\"\n",
    "    rdcols = [col for col in df if '.RD' in col]\n",
    "    for col in nb(rdcols):\n",
    "        pop = col.split(\".\")[0]\n",
    "        df[f'{pop}.RD'] = df[f'{pop}.DP'] - df[f'{pop}.AD']\n",
    "    return df\n",
    "\n",
    "def save_file(df, f):\n",
    "    \"\"\"Save file to background using one of the ipcluster engines so I can contiue working.\"\"\"\n",
    "    import pandas\n",
    "    df.to_csv(f, sep='\\t', index=False)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/03_maf-p05_RD-recalculated_both\n"
     ]
    }
   ],
   "source": [
    "# make dirs\n",
    "newdirs = {}\n",
    "for variety in ['both']:\n",
    "    newdirs[variety] = makedir(f'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/{dirdict[variety]}/03_maf-p05_RD-recalculated_{variety}')\n",
    "    print(newdirs[variety])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 4440.73it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 1416.71it/s]\n"
     ]
    }
   ],
   "source": [
    "recalced = {}\n",
    "for f,df in dfs.items():\n",
    "    recalced[f] = recalc_rd(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/combined_varieties/03_maf-p05_RD-recalculated_both/DF_pooled-varscan_all_bedfiles_SNP_both-varieties_maf_RD-recalculated.txt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save interior SNP data\n",
    "combfile = op.join(newdirs['both'], 'DF_pooled-varscan_all_bedfiles_SNP_both-varieties_maf_RD-recalculated.txt')\n",
    "save_file(combined_recalc, combfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f,df in recalced.items():\n",
    "    dst = f.replace(\"02_baseline_filtered_both\" ,\"03_maf-p05_RD-recalculated_both\").replace(\".txt\", \"_maf_RD-recalculated.txt\")\n",
    "    df.to_csv(dst, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter for MAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maf_filter(f):\n",
    "    \"\"\"filter MAF >= 0.01, write to file.\"\"\"\n",
    "    import pandas\n",
    "    import os\n",
    "    chunks = pandas.read_csv(f, sep='\\t', chunksize=10000)\n",
    "    dfs = []\n",
    "    for chunk in chunks:\n",
    "#         if len(dfs) % 5 == 0:\n",
    "#             update([op.basename(f), len(dfs)])\n",
    "        df = chunk[(chunk['AF'].astype(float) >= 0.01) & (chunk['AF'].astype(float) <= 0.99)].copy()\n",
    "        df['MAF'] = df['AF']\n",
    "        df.loc[df['AF'].astype(float) > 0.5, 'MAF'] = 1 - chunk['AF'][chunk['AF'].astype(float) > 0.5]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pandas.concat(dfs)\n",
    "    print(df.shape)\n",
    "    \n",
    "    out = f.replace(\".txt\", \"_maf.txt\").replace('02_baseline_filtered', '03_maf-p01_RD-recalculated')\n",
    "    makedir(os.path.dirname(out))\n",
    "    \n",
    "    df.to_csv(out, sep='\\t', index=False)\n",
    "    print('done!')\n",
    "dview['maf_filter'] = maf_filter\n",
    "dview['makedir'] = makedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs, get the baseline filtered files\n",
    "DIR = '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA'\n",
    "snpdir = op.join(DIR, 'DF_pooled/snpsANDindels')\n",
    "basedir = op.join(snpdir, '02_baseline_filtered')\n",
    "basefiles = [f for f in fs(basedir) if '.md5' not in f and 'INDEL' not in f]\n",
    "basefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = make_jobs(basefiles, maf_filter, lview)\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recalcuate RD\n",
    "\n",
    "Looking at our testdata (1 poolseq pop vs indSeq of same individuals), AD/DP was consistent with the frequency prediction from GATK. We saw that adjusting FREQ to AD / (AD + RD) decreased concordance between the two datasets. So that we are consistent with respect to uncorrected and corrected, I'm adjusting RD = DP - AD so we don't have to make adjustments in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonimports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/03_maf-p01_RD-recalculated'\n",
    "files = fs(DIR, endswith='maf.txt', exclude='README')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paralog and repeats, recalc RD\n",
    "def recalc_rd(df):\n",
    "    \"\"\"Recalculate RD so RD = DP - AD.\"\"\"\n",
    "    rdcols = [col for col in df if '.RD' in col]\n",
    "    for col in nb(rdcols):\n",
    "        pop = col.split(\".\")[0]\n",
    "        df[f'{pop}.RD'] = df[f'{pop}.DP'] - df[f'{pop}.AD']\n",
    "    return df\n",
    "    \n",
    "dfs = {}\n",
    "for f in files[:-1]:\n",
    "    print(f)\n",
    "    df = pd.read_table(f)\n",
    "    dfs[f] = recalc_rd(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipto_df(f, skipto, nrows, cols=None):\n",
    "    \"\"\"Retrieve dataframe so that all rows are captured when iterating.\n",
    "    \n",
    "    f = filename to open\n",
    "    skipto = row number to skip, read rows thereafter\n",
    "    nrows = how many rows to read from f\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    \n",
    "    if skipto == 0:\n",
    "        df = pandas.read_table(f, nrows=nrows-1)\n",
    "    else:\n",
    "        df = pandas.read_table(f, skiprows=range(1, skipto), nrows=nrows)\n",
    "    \n",
    "    if cols is not None:\n",
    "        if isinstance(cols, str):\n",
    "            cols = [cols]\n",
    "        df = df[cols].copy()\n",
    "    \n",
    "    return df\n",
    "dview['get_skipto_df'] = get_skipto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get linenums, includes header\n",
    "linenums = {}\n",
    "for f in files:\n",
    "    out = !wc -l $f\n",
    "    linenums[f] = int(out[0].split()[0])\n",
    "    print(op.basename(f), linenums[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in SNPs in parallel, \n",
    "nrows = 50000\n",
    "jobs = []\n",
    "count = 0\n",
    "for skipto in range(0, linenums[files[-1]], nrows):\n",
    "    num = str(count).zfill(4)\n",
    "    jobs.append(lview.apply_async(get_skipto_df, *(files[-1], skipto, nrows)))\n",
    "    count += 1\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalc_rd for SNPs\n",
    "dfs[files[-1]] = recalc_rd(pd.concat([j.r for j in jobs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f,df in dfs.items():\n",
    "    print(f)\n",
    "    newf = f.replace(\"_maf.txt\", \"_maf_RD-recalculated.txt\")\n",
    "    print('\\t', newf)\n",
    "    df.to_csv(newf, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose SNPs for GEA structure correction in baypass\n",
    "\n",
    "First choose loci for each variety (coastal and interior) then choose loci across both.\n",
    "\n",
    "for SNPs with MAF > 0.01 and for all pops: 40 < DP < 1000, randomly choose one snp per contig (for contigs > 1Kbp), then LD prune so no pairwise r2 > 99.9th percentile of r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonimports import *\n",
    "\n",
    "def get_mafdict(afs, roundto):\n",
    "    # bins for p52 depth\n",
    "    mafs = [round(1-float(af),roundto) if float(af) > 0.5 else round(float(af),roundto) for af in afs]\n",
    "    t = table(mafs)\n",
    "    mafsdict = OrderedDict()\n",
    "    for k in sorted(t):\n",
    "        newk = '%.10f' % k\n",
    "        if not newk in mafsdict:\n",
    "            mafsdict[newk] = t[k]\n",
    "        else:\n",
    "            mafsdict[newk] += t[k]\n",
    "    retdict = OrderedDict()\n",
    "    for newk,count in mafsdict.items():\n",
    "        retdict[newk] = count / sum(mafsdict.values())\n",
    "    print(len(retdict.keys()))\n",
    "\n",
    "    return retdict\n",
    "\n",
    "def subcategorybar(X, vals, width=.9):\n",
    "    n = len(vals)\n",
    "    _X = np.arange(len(X))\n",
    "    for i in range(n):\n",
    "        plt.bar(_X - width/2. + i/float(n)*width, vals[i], \n",
    "                width=width/float(n), align=\"edge\")   \n",
    "    plt.xticks(_X, X)\n",
    "\n",
    "def make_mafdict_fig(pruned_snpdict, all_snpdict, title=None):\n",
    "    fig = plt.figure(figsize=(25,5))\n",
    "    X = ['%.3f' % float(key) for key in sorted(keys(pruned_snpdict))]\n",
    "    Y = [pruned_snpdict[freq] for freq in sorted(keys(pruned_snpdict))]\n",
    "    Z = [all_snpdict[freq] for freq in sorted(keys(all_snpdict))]\n",
    "    subcategorybar(X, [Y,Z])\n",
    "    plt.legend(['pruned snps', 'all snps'],fontsize=20)\n",
    "    plt.ylabel('proportion of SNPs in category',size=20)\n",
    "    plt.xlabel('minor allele frequency',size=20)\n",
    "    plt.title(title, size=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snpdir = '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_depth(*args):\n",
    "    \"\"\"Filter min/max depth, reduce columns.\"\"\"\n",
    "    chunk = get_skipto_df(*args)\n",
    "    dpcols = [col for col in chunk.columns if '.DP' in col]\n",
    "    freqcols = [col for col in chunk.columns if '.FREQ' in col]\n",
    "    cols = ['CHROM', 'locus', 'AF', 'MAF'] + dpcols + freqcols\n",
    "    chunk = chunk[cols].copy()\n",
    "    for col in dpcols:\n",
    "        chunk = chunk[chunk[col] >= 20].copy()\n",
    "        chunk = chunk[chunk[col] < 1000].copy()\n",
    "    chunk.index = chunk['locus'].tolist()\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafdir = op.join(snpdir, '03_maf-p01_RD-recalculated')\n",
    "f = op.join(mafdir, 'DF_pooled-varscan_all_bedfiles_SNP_maf_RD-recalculated.txt')\n",
    "op.exists(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = !wc -l $f\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linenums = int(out[0].split()[0]) - 1\n",
    "linenums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linenums = 6728421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for depth in parallel as I read it in\n",
    "nrows = 50000\n",
    "jobs = []\n",
    "for skipto in range(0, linenums, nrows):\n",
    "    jobs.append(lview.apply_async(filter_depth, *(f, skipto, nrows)))\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = pd.concat([j.r for j in jobs])\n",
    "print(snps.shape)\n",
    "snps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luni(snps['CHROM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get a dict to distinguish interior vs coastal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envdata has variety ID\n",
    "envdata = pd.read_table('/data/projects/pool_seq/environemental_data/df_std_env-19variables.txt')\n",
    "envdata = envdata[envdata['our_id']==envdata['our_id']]  # removes irrelevant pop with our_id=nan\n",
    "\n",
    "pool2var = {}\n",
    "varlist = {}\n",
    "for row in envdata.index:\n",
    "    pool = envdata.loc[row, 'our_id']\n",
    "    variety = envdata.loc[row, 'Variety']\n",
    "    pool2var[pool] = variety\n",
    "    if variety not in varlist:\n",
    "        varlist[variety] = []\n",
    "    varlist[variety].append(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var,lst in varlist.items():\n",
    "    print(var, len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pool2var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take a look at missing data among varities (we filtered for missing data across both varieties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in snps.columns if '.' not in col]  # columns with info relevant to both varieties\n",
    "varsnps = {}\n",
    "for variety in uni(keys(varlist)):\n",
    "    varcols = [col for pop in varlist[variety] for col in snps.columns if col == f'{pop}.FREQ']\n",
    "    print(variety, len(varcols))\n",
    "    varsnps[variety] = snps[cols + varcols].copy()\n",
    "    print(variety, varsnps[variety].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varsnps['FDI'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varsnps['FDC'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview['varsnps'] = varsnps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varsnps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_missing_data(loci, variety=None):\n",
    "    df = varsnps[variety]\n",
    "    missingdata = []\n",
    "    for locus in loci:\n",
    "        sums = sum([1 for x in df.loc[locus, [col for col in df.columns if '.FREQ' in col]] if x==x])\n",
    "        missingdata.append(sums)\n",
    "    return missingdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = send_chunks(calc_missing_data, snps.index.tolist(), nrow(snps)/10, lview, {'variety': 'FDI'})\n",
    "jobs.extend(send_chunks(calc_missing_data, snps.index.tolist(), nrow(snps)/10, lview, {'variety': 'FDC'}))\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingdata = {}\n",
    "for i,j in enumerate(jobs):\n",
    "    if i < 10:\n",
    "        var = 'FDI'\n",
    "    else:\n",
    "        var = 'FDC'\n",
    "    if var not in missingdata:\n",
    "        missingdata[var] = []\n",
    "    for sums in j.r:\n",
    "        missingdata[var].append(1-(sums/len(varlist[var])))\n",
    "for var,lst in missingdata.items():\n",
    "    print(var, len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var,lst in missingdata.items():\n",
    "    plt.hist(lst)\n",
    "    plt.title(f'{var} missingdata')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce those snps on contigs > 1Kbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contig lengths\n",
    "lengths = pd.read_table('/data/database/DouglasFir_ref_genome/DF_ref_edit.fasta.length', header=None)\n",
    "lengths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = dict((contig,length) for (contig,length) in zip(lengths[0],lengths[1]) if length>1000)\n",
    "len(lens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqcols = [col for col in snps.columns if '.FREQ' in col]\n",
    "reduced = snps[snps['CHROM'].isin(list(lens.keys()))].copy()\n",
    "reduced = reduced.loc[:, freqcols + ['CHROM', 'AF', 'MAF']]\n",
    "reduced.shape, snps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(lens.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mafdict_fig(get_mafdict(reduced['AF'], roundto=2),\n",
    "                 get_mafdict(snps['AF'], roundto=2),\n",
    "                 title='Doug fir\\nafter filtering for contig length >1Kbp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce to no missing data  (can't, MAF spectrum doesn't match well with MAF spectrum of all data. Filter for % missing below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_col(col):\n",
    "#     \"\"\"Return list of loci with no missing data for pop.FREQ col.\"\"\"\n",
    "#     return reduced[~reduced[col].isnull()].index\n",
    "\n",
    "# dview['reduced'] = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # jobs = []\n",
    "# # for col in nb(freqcols):\n",
    "# #     jobs.append(reduce_col(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # send jobs to reduce_col\n",
    "# freqcols = [col for col in reduced.columns if '.FREQ' in col]\n",
    "# jobs = []\n",
    "# for col in freqcols:\n",
    "#     jobs.append(lview.apply_async(reduce_col, col))\n",
    "# watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine returns to get loci with no missing data across pop.FREQ cols\n",
    "# nomissingloci = snps.index.tolist()\n",
    "# for j in jobs:\n",
    "#     nomissingloci = list(set(nomissingloci).intersection(j.r))\n",
    "# len(nomissingloci), luni(nomissingloci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrow(reduced), nrow(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # combine returns to get loci with no missing data across pop.FREQ cols\n",
    "# # nomissingloci = snps.index.tolist()\n",
    "# # for j in jobs:\n",
    "# #     nomissingloci = list(set(nomissingloci).intersection(j))\n",
    "# # len(nomissingloci), luni(nomissingloci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reduce snp table to no missing loci\n",
    "# reduced = reduced[reduced.index.isin(nomissingloci)]\n",
    "# reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(nomissingloci), nrow(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrow(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(reduced['MAF'], bins=100)\n",
    "# plt.title('SNPs with no missing data\\nAnd 20 < DP < 1000\\nall contigs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(snps['MAF'],bins=100)\n",
    "# plt.title('all SNPs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to reduce to X% missing data ... Result: any filtering resulted in a visibly different MAF spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_perc(loci, perc=0.10):\n",
    "    \"\"\"Return only loci with missing data <= perc arg.\n",
    "    freqcols = all cols with .FREQ in col name\n",
    "    \"\"\"\n",
    "    import pandas\n",
    "    from tqdm.notebook import tqdm as nb\n",
    "\n",
    "    keep = []\n",
    "    for locus in nb(loci):\n",
    "        sums = sum([1 for freq in reduced.loc[locus,freqcols] if freq!=freq])\n",
    "        if sums/len(freqcols) <= perc:\n",
    "            keep.append(locus)\n",
    "    return keep\n",
    "dview['reduced'] = reduced\n",
    "dview['freqcols'] = freqcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(reduced)/len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luni(reduced['CHROM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> SETTING PERC TO 0.25 MEANS NO LOCI WERE FILTERED BY MAF\n",
    "    \n",
    "    if I tried filtering < 25% missing data, the MAF spectrum looked weird, leaving the code inplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = send_chunks(filter_perc, reduced.index.tolist(), nrow(reduced)/len(lview), lview, {'perc':0.25})\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_above = []\n",
    "for j in jobs:\n",
    "    perc_above.extend(j.r)\n",
    "len(perc_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reduced.shape)\n",
    "reduced = reduced[reduced.index.isin(perc_above)].copy()\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what I used (pipeline default)\n",
    "make_mafdict_fig(get_mafdict(reduced['AF'], roundto=2),\n",
    "                 get_mafdict(snps['AF'], roundto=2),\n",
    "                 title='Doug fir\\nafter filtering for <= 25 % missing data (pipeline default)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is when trying to filter for less missing data than default (eg perc=0.1)\n",
    "make_mafdict_fig(get_mafdict(reduced['AF'], roundto=2),\n",
    "                 get_mafdict(snps['AF'], roundto=2),\n",
    "                 title='Doug fir\\nafter filtering for perc missing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### choose one snp per contig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_loci(chrom_list):\n",
    "    \"\"\"For each chrom in chrom_list, randomly choose 1 snp.\"\"\"\n",
    "    from random import shuffle\n",
    "    from tqdm.notebook import tqdm as nb\n",
    "    \n",
    "    loci = []\n",
    "    for chrom in nb(chrom_list):\n",
    "        chrom_snps = list(reduced[reduced['CHROM']==chrom].index)\n",
    "        shuffle(chrom_snps)\n",
    "        loci.append(chrom_snps[0])\n",
    "\n",
    "    return loci\n",
    "dview['reduced'] = pd.DataFrame(reduced['CHROM'])  # reload reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_chroms = uni(reduced['CHROM'])\n",
    "len(uni_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uni_chroms)/len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to choose_random_loci\n",
    "# jobs = send_chunks(choose_random_loci, uni_chroms, len(uni_chroms)/len(lview), lview)\n",
    "watch_async(jobs, phase='len(uni_chrom) = %s' % len(uni_chroms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomloci = []\n",
    "for j in jobs:\n",
    "    randomloci.extend(j.r)\n",
    "len(randomloci) == len(uni_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(randomloci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = reduced[reduced.index.isin(randomloci)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mafdict_fig(get_mafdict(reduced['AF'], roundto=2),\n",
    "                 get_mafdict(snps['AF'], roundto=2),\n",
    "                 title='Doug fir\\nafter choose one SNP per contig for contigs > 1Kb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get an idea of r2 values so we can determine an empirical high-end cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # skip choosing 1 per contig to see what happens\n",
    "# randomloci = list(nomissingloci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview,dview = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getfreqs(myloci):\n",
    "    \"\"\"Get the population ALT frequency as a float (reported as a str in table).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples\n",
    "        - first element = locus name\n",
    "        - second element = pandas.Series\n",
    "    \"\"\"\n",
    "    rets = []\n",
    "    for locus in myloci:\n",
    "        rets.append((locus, reduced.loc[locus, freqcols].str.replace(\"%\", \"\").astype(float) / 100))\n",
    "    return rets\n",
    "dview['getfreqs'] = getfreqs\n",
    "dview['freqcols'] = freqcols\n",
    "dview['reduced'] = reduced[freqcols]\n",
    "\n",
    "def getr2(myloci):\n",
    "    from scipy.stats import pearsonr\n",
    "    from numpy import logical_or\n",
    "    from tqdm.notebook import tqdm as nb\n",
    "\n",
    "    freqs = dict((locus, freqs) for (locus,freqs) in getfreqs(myloci))\n",
    "\n",
    "    r2vals = []\n",
    "    i = 0\n",
    "    for locusi in nb(myloci):\n",
    "        for j,locusj in enumerate(myloci):\n",
    "            if i < j:\n",
    "                nas = logical_or(freqs[locusi].isnull(), freqs[locusj].isnull())\n",
    "                r2 = pearsonr(freqs[locusi][~nas], freqs[locusj][~nas])[0]**2\n",
    "                r2vals.append(r2)\n",
    "        i += 1\n",
    "    return r2vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs = send_chunks(getr2, randomloci, 250, lview)\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2vals = []\n",
    "for j in jobs:\n",
    "    r2vals.extend(j.r)\n",
    "plt.hist(r2vals, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 99.9th percentile\n",
    "r2thresh = sorted(r2vals)[math.ceil(len(r2vals)*.999)]\n",
    "r2thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is 99.99th percentile?\n",
    "sorted(r2vals)[math.ceil(len(r2vals)*.9999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is 99th percentile?\n",
    "sorted(r2vals)[math.ceil(len(r2vals)*.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentiles are too high, let's see what 0.2 looks like\n",
    "r2thresh = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(sorted(r2vals)):\n",
    "    if x > r2thresh:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i/len(r2vals)  # percentile of r2thresh in r2vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(r2vals)[math.ceil(len(r2vals)*(i/len(r2vals)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LD prune random loci¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(randomloci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps.shape, reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqcols = [col for col in snps if '.FREQ' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict((locus, freqs) for (locus,freqs) in getfreqs(randomloci))\n",
    "dview['freqs'] = freqs\n",
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview['reduced'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_em(compareto, locusi=None, r2thresh=0.2):\n",
    "    from scipy.stats import pearsonr\n",
    "    from numpy import logical_or\n",
    "    \n",
    "    drop = []\n",
    "    for locusj in compareto:\n",
    "        nas = logical_or(freqs[locusi].isnull(), freqs[locusj].isnull())\n",
    "        r2 = pearsonr(freqs[locusi][~nas], freqs[locusj][~nas])[0]**2\n",
    "#         print('r2 = ', r2)\n",
    "        if r2 > r2thresh:\n",
    "            drop.append(locusj)\n",
    "\n",
    "    return locusi,drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send locus along with all needed loci for pairwise comparisons to engines\n",
    "jobs = []\n",
    "i = 0\n",
    "for locusi in tnb(randomloci):\n",
    "    tosend = randomloci[i+1:]\n",
    "    jobs.append(lview.apply_async(prune_em, tosend, **{'r2thresh':r2thresh, 'locusi':locusi}))\n",
    "    i += 1\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found = {-1: None}  # comment this out after running this the first time in case I want to interrupt and restart\n",
    "# keep = list(randomloci)  # comment this out after running this the first time in case I want to interrupt and restart\n",
    "maxx = -1\n",
    "while maxx < (len(randomloci) - 1):\n",
    "    i = 0\n",
    "    for j in tnb(jobs):\n",
    "        if j.ready() and i-1 in found.keys() and i not in found.keys():\n",
    "            found[i] = True\n",
    "            try:\n",
    "                locusi, drop = j.r\n",
    "            except: # CancelledError\n",
    "                continue\n",
    "            if locusi in keep:\n",
    "                for locusj in drop:\n",
    "                    if locusj in keep:\n",
    "                        job_idx = randomloci.index(locusj)\n",
    "    #                     print('\\tcanceling ', job_idx, '. ', locusj, randomloci[job_idx])\n",
    "                        jobs[job_idx].cancel()\n",
    "                        keep.remove(locusj)\n",
    "        i += 1\n",
    "    maxx = max(found.keys())\n",
    "    if maxx+1 < len(randomloci):\n",
    "        update([(jobs[maxx+1].ready(), maxx, len(found.keys()), len(keep))])\n",
    "        time.sleep(10)\n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonimports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkldump(keep, '/data/home/lindb/temp_dougfir_gea_pruned.pkl')\n",
    "keep = pklload('/data/home/lindb/temp_dougfir_gea_pruned.pkl')\n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keeping = snps[snps['locus'].isin(keep)].copy()\n",
    "keeping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mafdict_fig(get_mafdict(keeping['AF'], roundto=2),\n",
    "                 get_mafdict(snps['AF'], roundto=2),\n",
    "                 title='Doug fir\\nafter pruing for r2 > 0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is when I had used a lower threshold of missing data.\n",
    "make_mafdict_fig(pruned_mafdict, snp_mafdict, 'Doug fir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luni(keeping['CHROM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([lens[chrom] for chrom in keeping['CHROM']], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([lens[chrom] for chrom in snps['CHROM'] if chrom in lens], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lens), luni(snps['CHROM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "baydir = makedir(op.join(mafdir, 'baypass'))\n",
    "pkldump(keep, op.join(baydir, 'lessthan10perc-missing-data_20-dp-1000_random-snps_1-per-contig-gt1Kbp_r2-lessthan-p30.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(keeping['MAF'], bins=100)\n",
    "plt.title('Structure SNPs')\n",
    "plt.xlabel('MAF')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(snps['MAF'], bins=100)\n",
    "plt.title('All SNPs')\n",
    "plt.xlabel('MAF')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get full snps df to reduce to loci in keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linenums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all SNPs in parallel\n",
    "nrows = 50000\n",
    "jobs = []\n",
    "for skipto in range(0, linenums, nrows):\n",
    "    jobs.append(lview.apply_async(get_skipto_df, *(f, skipto, nrows)))\n",
    "watch_async(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = pd.concat([j.r for j in jobs])\n",
    "snps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps.index = snps['locus'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = snps.loc[keep, :].copy()\n",
    "filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baydir = '/data/projects/pool_seq/DF_datasets/DF_pooled_GEA/DF_pooled/snpsANDindels/03_maf-p01_RD-recalculated/baypass'\n",
    "pkldump(keep, op.join(baydir,\n",
    "                      'baseline-missing-data_20-dp-1000_random-snps_1-per-contig-gt1Kbp_r2-lessthan-p20.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # moved this and all files like it to baydir/bad_maf_spectrum\n",
    "# filtered.to_csv(op.join(baydir, 'lessthan10perc-missing-data_20-dp-1000_random-snps_1-per-contig-gt1Kbp_r2-lessthan-p30_table.txt'),\n",
    "#                 sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.to_csv(op.join(baydir, 'baseline-missing-data_20-dp-1000_random-snps_1-per-contig-gt1Kbp_r2-lessthan-p20_table.txt'),\n",
    "                sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview['df'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
